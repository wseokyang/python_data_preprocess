{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 데이터 불러오기 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "## 1. DB connection과 SQL 쿼리 실행(시연, 참고) \n",
    "\n",
    "일반적으로 **Database**를 활용할 때 SQL Client에 접속해서 **SQL query**를 실행합니다. 보안상 이슈가 있지만, Python에서 DB에 직접 연결한 다음 SQL query를 실행할 수 있습니다. **pandas**의 *read_sql()*를 활용하면 결과물을 바로 DataFrame 형식으로 받아와 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치\n",
    "!pip install PyMySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection 생성\n",
    "    ## 강사 집에 임시로 만든 데이터베이스에 접속\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=\"dataart.asuscomm.com\",\n",
    "    port = 3306, \n",
    "    user=\"test1\",\n",
    "    passwd=\"1234\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM sales.customers\"\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. API의 활용 및 json의 활용(시연, 참고)\n",
    "\n",
    "API로 데이터를 받아올 수도 있습니다. API는 정해진 약속에 따라 데이터를 요청하는 방법으로 결과 데이터는 json 등의 형식으로 들어옵니다. json 형식은 적절한 방법으로 DataFrame 형식으로 바꿔 활용할 수 있습니다.  \n",
    "\n",
    "기상청 API를 활용해서 **서초동**의 날씨 예보를 데이터로 가져와 보겠습니다.\n",
    " * 참고: [기상청 API](https://www.data.go.kr/tcs/dss/selectApiDataDetailView.do?publicDataPk=15084084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기준 날짜, 기준 시간 설정\n",
    "from datetime import datetime, timedelta\n",
    "time1 = datetime.now()\n",
    "bs_date = time1.strftime('%Y%m%d')\n",
    "time2 = time1 - timedelta(minutes=12)\n",
    "bs_time = ['0800', '1100', '1400', '1700'][(time2.hour>=11) + (time2.hour>=14) + (time2.hour>=17)]\n",
    "bs_date, bs_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기상청 \n",
    "import requests\n",
    "\n",
    "url = 'http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst'\n",
    "params ={'serviceKey':'1HcH5lG+R0QgBAxXB3jcBBOkcUbN61xgza6VbyQruMEVrTmgX1JQ9LsIeUFBHLG0/12ZmN5uKAfhPDMPjk+gaQ==', \n",
    "         'numOfRows':'1000', \n",
    "         'dataType':'JSON', # XML도 활용 가능\n",
    "         'base_date':bs_date, \n",
    "         'base_time':bs_time, \n",
    "         'nx':'61', \n",
    "         'ny':'125'} \n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key 활용 부분 선택 \n",
    "json.loads(response.content)['response']['body']['items']['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(json.loads(response.content)['response']['body']['items']['item'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3.  BeautifulSoup을 활용한 웹스크랩/웹크롤링\n",
    "\n",
    "\n",
    "*BeautifulSoup*은 네이버 뉴스처럼 별도의 로그인 절차 등이 없는 열린 페이지를 스크랩할 때 주로 활용합니다.  \n",
    "<br> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치\n",
    "!pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "## 3.1. 네이버 뉴스 스크랩 예제\n",
    "\n",
    "네이버 등 포털사이트의 검색 결과 등을 간단히 스크랩 할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 검색 후 URL 복사, 붙여넣기\n",
    "url1 = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90'\n",
    "page1 = urlopen(url1)\n",
    "soup1 = BeautifulSoup(page1, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 클래스를 활용한 선택 \n",
    "title_list = soup1.find_all(class_='news_tit')\n",
    "title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째만 선택 후 구조 확인\n",
    "title_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .attrs를 활용한 어트리뷰트 확인\n",
    "title_list[0].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어트리뷰트를 활용한 선택\n",
    "title_list[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for를 활용한 전체 기사의 제목만  리스트 형식으로 추출\n",
    "# [x.get_text() for x in title_list]\n",
    "[x['title'] for x in title_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "for문을 활용해서 페이지를 바꿔가며 제목을 추가하는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잠시멈춤을 위한 sleep 불러오기\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 만들기\n",
    "newstitle = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 웹사이트의 특성을 활용한 url 지정 후 스크랩\n",
    "for i in range(0, 5):\n",
    "    stt_value = str(1 + i*10)\n",
    "    url_ = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&start=' + stt_value\n",
    "    page_ = urlopen(url_)\n",
    "    soup_ = BeautifulSoup(page_, 'html.parser')\n",
    "    title_list_ = soup_.find_all(class_='news_tit')\n",
    "    \n",
    "    newstitle.extend([x['title'] for x in title_list_])\n",
    "    print('{}번째 페이지 스크랩 완료'.format(i+1))\n",
    "    sleep(3) ## 3초 멈춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newstitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'title':newstitle})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [참고] 하나의 파일로 저장 후 활용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime \n",
    "\n",
    "url1 = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90'\n",
    "page1 = urlopen(url1)\n",
    "soup1 = BeautifulSoup(page1, 'html.parser')\n",
    "\n",
    "# 클래스를 활용한 선택 \n",
    "title_list = soup1.find_all(class_='news_tit')\n",
    "df_temp = pd.DataFrame({'title':[x['title'] for x in title_list]})\n",
    "\n",
    "filename1 = datetime.now().strftime(\"title_%Y%m%d-%H%M%S.txt\")\n",
    "df_temp.to_csv(filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXE 만들기\n",
    "# auto-py-to-exe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [참고] Selenium의 활용\n",
    "\n",
    "[python selenium](https://www.google.com/search?q=python+selenium)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
